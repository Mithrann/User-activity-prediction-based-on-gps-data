# -*- coding: utf-8 -*-
"""Geolife2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MfqGT6oMUD3BDPzUc1Op1rbRqvAZhXEm
"""

pip install geopy

import shutil
import random
import zipfile
import os
import pandas as pd
import numpy as np
import glob
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder
from geopy.distance import geodesic

zip_path = '/content/Geolife_Trajectories_1.3.zip'
extract_folder = '/content/geolife_data/'
num_files_to_extract = 50

if os.path.exists(extract_folder):
    shutil.rmtree(extract_folder)  # Delete the folder and its contents
os.makedirs(extract_folder, exist_ok=True)

with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    plt_files_in_zip = [f for f in zip_ref.namelist() if f.endswith('.plt')]
    print("Total .plt files in ZIP:", len(plt_files_in_zip))
    sampled_files = random.sample(plt_files_in_zip, num_files_to_extract)
    for file in sampled_files:
        zip_ref.extract(file, extract_folder)

plt_files = glob.glob(os.path.join(extract_folder, '**', '*.plt'), recursive=True)
print("Extracted .plt files:", len(plt_files))

file_paths = glob.glob('/content/geolife/Data/*/Trajectory/*.plt')

num_files_to_load = 50

sampled_files = random.sample(plt_files, num_files_to_load)
dataframes = []

for file in plt_files:
    df = pd.read_csv(file, delimiter=',', header=None,
                     names=['latitude', 'longitude', 'zero', 'altitude', 'date', 'time'],
                     skiprows=6)
    dataframes.append(df)

all_data = pd.concat(dataframes, ignore_index=True)

all_data['timestamp'] = pd.to_datetime(all_data['date'] + ' ' + all_data['time'], errors='coerce')

all_data = all_data[['latitude', 'longitude', 'altitude', 'timestamp']]

all_data = pd.concat([all_data], ignore_index=True)

print(all_data.info())
print(all_data.head())

all_data['activity'] = np.random.choice(['walk', 'bike', 'car', 'train'], size=len(all_data))

label_encoder = LabelEncoder()
all_data['activity_encoded'] = label_encoder.fit_transform(all_data['activity'])

all_data['day_of_week'] = all_data['timestamp'].dt.dayofweek
all_data['hour_of_day'] = all_data['timestamp'].dt.hour
all_data['minute_of_hour'] = all_data['timestamp'].dt.minute
all_data['second_of_minute'] = all_data['timestamp'].dt.second

X = all_data[['latitude', 'longitude', 'altitude', 'day_of_week', 'hour_of_day', 'minute_of_hour']]
y = all_data['activity_encoded']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

print("X_train shape:", X_train.shape)
print("y_train shape:", y_train.shape)
print("X_test shape:", X_test.shape)
print("y_test shape:", y_test.shape)

from geopy.distance import geodesic
import pandas as pd

def calculate_distance(lat1, lon1, lat2, lon2):
    if not (-90 <= lat1 <= 90) or not (-90 <= lat2 <= 90):
        return None
    if not (-180 <= lon1 <= 180) or not (-180 <= lon2 <= 180):
        return None

    return geodesic((lat1, lon1), (lat2, lon2)).meters

distances = []
speeds = []

all_data_subset = all_data.head(10)

for i in range(1, len(all_data_subset)):
    lat1, lon1 = all_data_subset.iloc[i - 1][['latitude', 'longitude']]
    lat2, lon2 = all_data_subset.iloc[i][['latitude', 'longitude']]
    timestamp1 = all_data_subset.iloc[i - 1]['timestamp']
    timestamp2 = all_data_subset.iloc[i]['timestamp']

    print(f"Row {i}: Lat1={lat1}, Lon1={lon1}, Lat2={lat2}, Lon2={lon2}")

    distance = calculate_distance(lat1, lon1, lat2, lon2)

    if distance is None:
        print(f"Skipping row {i} due to invalid coordinates: Lat1={lat1}, Lon1={lon1}, Lat2={lat2}, Lon2={lon2}")
        distances.append(None)
        speeds.append(None)
        continue

    time_diff = timestamp2 - timestamp1
    speed = distance / time_diff.total_seconds() if time_diff.total_seconds() > 0 else 0
    distances.append(distance)
    speeds.append(speed)

all_data_subset['distance'] = [None] + distances
all_data_subset['speed'] = [None] + speeds

invalid_rows = all_data_subset[all_data_subset['distance'].isnull()]
print(f"Invalid rows in the subset of the dataset: {invalid_rows}")

speed_threshold = 1.0

if 'speed' not in all_data.columns:
    all_data['speed'] = [0] * len(all_data)

all_data['speed_kmh'] = all_data['speed'] * 3.6

all_data['stay_point'] = all_data['speed_kmh'] < speed_threshold

from sklearn.cluster import MiniBatchKMeans

kmeans = MiniBatchKMeans(n_clusters=3, batch_size=10000)  # Example parameters
coords = all_data[['latitude', 'longitude']].values
kmeans.fit(coords)
all_data['cluster'] = kmeans.labels_

stay_points = all_data[all_data['stay_point'] == True]
print(stay_points[['latitude', 'longitude', 'timestamp', 'stay_point', 'cluster']])

distances = np.linalg.norm(coords - kmeans.cluster_centers_[all_data['cluster']], axis=1)

threshold = np.percentile(distances, 95)

outliers = all_data[distances > threshold]

print("Noise points detected (Outliers):")
print(outliers[['latitude', 'longitude', 'timestamp', 'cluster']])

accuracy_scores = {}

models = {
    'Random Forest': RandomForestClassifier(),
    }

for model_name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    accuracy_scores[RandomForestClassifier()] = accuracy

print("Accuracy Scores:", accuracy_scores)

models = {
    'K-Nearest Neighbors': KNeighborsClassifier()
}

for model_name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    accuracy_scores[KNeighborsClassifier()] = accuracy

print("Accuracy Scores:", accuracy_scores)

print(f"Confusion Matrix for {RandomForestClassifier()}:")
 cm = confusion_matrix(y_test, y_pred)
 print(cm)

print(f"Confusion Matrix for {KNeighborsClassifer()}:")
 cm = confusion_matrix(y_test, y_pred)
 print(cm)

sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)
plt.title(f'Confusion Matrix: {RandomForestClassifier()}')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)
plt.title(f'Confusion Matrix: {KNeighborsClassifier()}')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

rf_model = RandomForestClassifier(random_state=42)
rf_model.fit(X_train, y_train)
y_pred_rf = rf_model.predict(X_test)
accuracy_rf = accuracy_score(y_test, y_pred_rf)

knn_model = KNeighborsClassifier()
knn_model.fit(X_train, y_train)
y_pred_knn = knn_model.predict(X_test)
accuracy_knn = accuracy_score(y_test, y_pred_knn)

accuracy_scores = {
    "Random Forest": accuracy_rf,
    "K-Nearest Neighbors": accuracy_knn
}

plt.figure(figsize=(10, 6))
plt.bar(accuracy_scores.keys(), accuracy_scores.values(), color='skyblue')
plt.title('Accuracy Comparison of Models')
plt.ylabel('Accuracy')
plt.xticks(rotation=45)
plt.show()

all_data['duration'] = (all_data['timestamp'] - all_data['timestamp'].shift()).fillna(pd.Timedelta(seconds=0))
activity_duration = all_data.groupby('activity')['duration'].sum()
activity_duration.plot(kind='bar', figsize=(10, 6), color='lightcoral')
plt.title('Total Duration of Activities')
plt.ylabel('Total Duration (Time)')
plt.xlabel('Activity Mode')
plt.show()

plt.figure(figsize=(10, 6))
plt.scatter(all_data['longitude'], all_data['latitude'], c=all_data['stay_point'], cmap='coolwarm', label='Stay Points')
plt.title('Stay Points Detection')
plt.xlabel('Longitude')
plt.ylabel('Latitude')
plt.show()

stay_point_percentage = (all_data['stay_point'].sum() / len(all_data)) * 100
print(f"Percentage of stay points: {stay_point_percentage:.2f}%")

for cluster_label in np.unique(all_data['cluster']):
    cluster_points = all_data[all_data['cluster'] == cluster_label]
    plt.scatter(cluster_points['latitude'], cluster_points['longitude'],
                s=10, label=f'Cluster {cluster_label}')

plt.scatter(outliers['latitude'], outliers['longitude'], color='red',
            s=10, label='Noise (Outliers)', marker='x')

plt.title('MiniBatchKMeans Clustering with Noise Detection')
plt.xlabel('Latitude')
plt.ylabel('Longitude')
plt.legend()
plt.show()

print("Noise points detected (Outliers):")
print(outliers[['latitude', 'longitude', 'timestamp', 'cluster']])

noise_percentage = (len(outliers) / len(all_data)) * 100
print(f"Percentage of noise points (outliers): {noise_percentage:.2f}%")

activity_by_time = all_data.groupby(['hour_of_day', 'activity']).size().unstack()
activity_by_time.plot(kind='bar', stacked=True, figsize=(10, 6))
plt.title('Activity Mode Distribution by Hour of Day')
plt.ylabel('Activity Count')
plt.xlabel('Hour of Day')
plt.show()

all_data['activity_length'] = all_data['activity'].apply(len)
print(all_data[['activity', 'activity_length']].drop_duplicates())

all_data['activity'] = all_data['activity'].str.lower()
print(all_data['activity'].unique())
activity_counts = all_data['activity'].value_counts()
print(activity_counts)

print("NaN values in activity column:", all_data['activity'].isna().sum())
print("Empty strings in activity column:", all_data['activity'].str.strip().eq('').sum())

car_usage = all_data[all_data['activity'].str.contains('car', case=False, na=False)]
bike_usage = all_data[all_data['activity'].str.contains('bike', case=False, na=False)]
train_usage = all_data[all_data['activity'].str.contains('train', case=False, na=False)]
walk_usage = all_data[all_data['activity'].str.contains('walk', case=False, na=False)]

print(f"Car Usage: {len(car_usage)}")
print(f"Bike Usage: {len(bike_usage)}")
print(f"Train Usage: {len(train_usage)}")
print(f"Walk Usage: {len(walk_usage)}")

usage_counts = {
    'Car': len(car_usage),
    'Bike': len(bike_usage),
    'Train': len(train_usage),
    'Walk': len(walk_usage)
}

print(usage_counts)

plt.figure(figsize=(10, 6))
plt.bar(usage_counts.keys(), usage_counts.values(), color='lightblue')
plt.title('Activity Mode Usage Comparison')
plt.ylabel('Count')
plt.xlabel('Activity Mode')
plt.show()

activity_usage = all_data['activity'].value_counts()
activity_usage.plot(kind='bar', figsize=(10, 6), color='lightgreen')
plt.title('Activity Mode Usage')
plt.ylabel('Count')
plt.xlabel('Activity Mode')
plt.show()

all_data['duration'] = (all_data['timestamp'] - all_data['timestamp'].shift()).fillna(pd.Timedelta(seconds=0))
activity_duration = all_data.groupby('activity')['duration'].sum()
activity_duration.plot(kind='bar', figsize=(10, 6), color='lightcoral')
plt.title('Total Duration of Activities')
plt.ylabel('Total Duration (Time)')
plt.xlabel('Activity Mode')
plt.show()

